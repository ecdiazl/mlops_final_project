{
  "pipelineSpec": {
    "components": {
      "comp-data-preprocessing": {
        "executorLabel": "exec-data-preprocessing",
        "inputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "test_dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "train_dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-read-bigquery-table": {
        "executorLabel": "exec-read-bigquery-table",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "table_id": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-data-preprocessing": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "data_preprocessing"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas==1.3.5' 'scikit-learn==1.0.2' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef data_preprocessing(\n    dataset: Input[Dataset],\n    train_dataset: Output[Dataset],\n    test_dataset: Output[Dataset],\n):\n    \"\"\"Preprocesses tabular data for training.\"\"\"\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder, StandardScaler\n\n    # lee los datos de entrada\n    df = pd.read_csv(dataset.path)\n\n    # separamos el target del resto de las variables\n    X, y = df.iloc[:,1:-1], df[['target']]\n    columns_x = X.columns\n\n    # escalamos las variables\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    X = pd.DataFrame(X, columns=columns_x)\n\n    # concatenamos las variables escaladas con el target\n    df_scaled = pd.concat([X, y], axis=1)\n\n    # Split the data into train and test sets.\n    train_df, test_df = train_test_split(df_scaled, test_size=0.2, random_state=42)\n\n    train_df.to_csv(train_dataset.path, index=False)\n    test_df.to_csv(test_dataset.path, index=False)\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-read-bigquery-table": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "read_bigquery_table"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery[pandas]==3.10.0' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef read_bigquery_table(\n    project_id: str, \n    dataset_id: str, \n    table_id: str,\n    dataset: Output[Dataset],\n):\n    \"\"\"\n    Lee datos de una tabla de BigQuery y devuelve alg\u00fan resultado como ejemplo.\n\n    Args:\n    project_id: ID del proyecto en GCP.\n    dataset_id: ID del conjunto de datos en BigQuery.\n    table_id: ID de la tabla en el conjunto de datos.\n\n    Returns:\n    Una cadena que representa una parte de los datos le\u00eddos, por ejemplo.\n    \"\"\"\n    from google.cloud import bigquery\n\n    # Crea un cliente de BigQuery.\n    client = bigquery.Client(project=project_id)\n\n    # Define la consulta para seleccionar todos los datos de la tabla.\n    query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_id}` \"\n\n    # configuramos la consulta\n    job_config = bigquery.QueryJobConfig()\n    query_job = client.query(query=query, job_config=job_config)\n\n    # Convierte los resultados en un dataframe de pandas y\n    # escribe los resultados en un archivo CSV.\n    df = query_job.result().to_dataframe()\n    df.to_csv(dataset.path, index=False)\n\n"
            ],
            "image": "python:3.7"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "mlops-fp-pipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "data-preprocessing": {
            "cachingOptions": {},
            "componentRef": {
              "name": "comp-data-preprocessing"
            },
            "dependentTasks": [
              "read-bigquery-table"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset",
                    "producerTask": "read-bigquery-table"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "data-preprocessing"
            }
          },
          "read-bigquery-table": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-read-bigquery-table"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "data_mlops_fp"
                    }
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "mlops-final-project-412223"
                    }
                  }
                },
                "table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "tb_mlops_fp"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "read-bigquery-table"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.22"
  },
  "runtimeConfig": {}
}